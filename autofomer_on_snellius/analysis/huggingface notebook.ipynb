{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation on notebook by Huggingface on effectiveness transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part\n",
    "The blog says that the Autoformer paper claims some results in the MASE metric, but these are not present in the paper. This is code to calculate the MASE results ourselves to verify these claims. Do note that the blog only mentions results from the univariate autoformer and say that multivariate performs worse. We only have multivariate autoformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoformer\n",
    "from utils.tools import dotdict\n",
    "import torch\n",
    "\n",
    "def obtain_autoformer(pred_len, dataset):\n",
    "    assert dataset in [\"ECL\", \"Exchange\"]\n",
    "    assert pred_len in [96, 192, 336, 720]\n",
    "\n",
    "    args = dotdict()\n",
    "    args.pred_len = pred_len\n",
    "\n",
    "    if dataset == \"ECL\":\n",
    "        args.enc_in = 321\n",
    "        args.dec_in = 321\n",
    "        args.c_out = 321\n",
    "    elif dataset == \"Exchange\":\n",
    "        args.enc_in = 8\n",
    "        args.dec_in = 8\n",
    "        args.c_out = 8\n",
    "\n",
    "    args.target = 'OT'\n",
    "    args.des = 'train'\n",
    "    args.dropout = 0.05\n",
    "    args.num_workers = 10\n",
    "    args.gpu = 0\n",
    "    args.lradj = 'type1'\n",
    "    args.devices = '0'\n",
    "    args.use_gpu = False\n",
    "    args.use_multi_gpu = False\n",
    "    args.freq = 'h'\n",
    "    args.checkpoints = './checkpoints/'\n",
    "    args.bucket_size = 4\n",
    "    args.n_hashes = 4\n",
    "    args.is_trainging = True\n",
    "    args.data = 'custom'\n",
    "    args.features = 'M'\n",
    "    args.seq_len = 96\n",
    "    args.label_len = 48\n",
    "    args.e_layers = 2\n",
    "    args.d_layers = 1\n",
    "    args.n_heads = 8\n",
    "    args.factor = 1\n",
    "    args.d_model = 512\n",
    "    args.des = 'Exp'\n",
    "    args.itr = 1\n",
    "    args.d_ff = 2048\n",
    "    args.moving_avg = 25\n",
    "    args.factor = 3\n",
    "    args.distil = True\n",
    "    args.output_attention = False\n",
    "    args.embed = 'timeF'\n",
    "\n",
    "    autoformer_path = f\"/Users/angelavansprang/Documents/PhD/transformers for time series/Autoformer/checkpoints/{dataset}_96_{pred_len}_Autoformer_custom_ftM_sl96_ll48_pl{pred_len}_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\"\n",
    "\n",
    "    autoformer = Autoformer.Model(args).float()\n",
    "    autoformer.load_state_dict(torch.load(autoformer_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    autoformer.eval()\n",
    "\n",
    "    return autoformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def obtain_test_loader_electricity(pred_len, seq_len = 96): \n",
    "    label_len = 48\n",
    "\n",
    "    data_set_electricity = Dataset_Custom(\n",
    "        root_path = \"/Users/angelavansprang/Documents/PhD/transformers for time series/Autoformer/dataset/electricity/\",\n",
    "        data_path = \"electricity.csv\",\n",
    "        flag=\"train\",\n",
    "        size=[seq_len, label_len, pred_len], # seq_len, label_len, pred_len\n",
    "        features=\"M\",\n",
    "        target=\"OT\", #default\n",
    "        timeenc=1,\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    data_loader_electricity = DataLoader(\n",
    "        data_set_electricity,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return data_loader_electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def obtain_test_loader_exchange(pred_len, seq_len = 96): \n",
    "    label_len = 48\n",
    "\n",
    "    data_set_exchange = Dataset_Custom(\n",
    "        root_path = \"/Users/angelavansprang/Documents/PhD/transformers for time series/Autoformer/dataset/exchange_rate/\",\n",
    "        data_path = \"exchange_rate.csv\",\n",
    "        flag=\"test\",\n",
    "        size=[seq_len, label_len, pred_len], # seq_len, label_len, pred_len\n",
    "        features=\"M\",\n",
    "        target=\"OT\", #default\n",
    "        timeenc=1,\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    data_loader_exchange = DataLoader(\n",
    "        data_set_exchange,\n",
    "        batch_size=10,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return data_loader_exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 96\n",
    "label_len = 48\n",
    "pred_len = 720\n",
    "\n",
    "dataloader_elec = obtain_test_loader_electricity(pred_len=pred_len, seq_len=seq_len)\n",
    "autoformer_elec = obtain_autoformer(pred_len=pred_len, dataset=\"ECL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import metric\n",
    "\n",
    "def test(model, test_loader, label_len=48):\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        # folder_path = './test_results/' + setting + '/'\n",
    "        # if not os.path.exists(folder_path):\n",
    "        #     os.makedirs(folder_path)\n",
    "\n",
    "        print(\"max_i == \", len(test_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                # print(\"data shape: \", batch_x.shape, batch_y.shape, batch_x_mark.shape, batch_y_mark.shape)\n",
    "                batch_x = batch_x.float()\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float()\n",
    "                batch_y_mark = batch_y_mark.float()\n",
    "\n",
    "                outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark) # assumption model is autoformer\n",
    "\n",
    "                pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                true = batch_y[:, :-label_len, :]  # remove init for forecasting batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"{i}/{len(test_loader)}\")\n",
    "\n",
    "                if i == 100: #== 100\n",
    "                    break\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        # print('test shape:', preds.shape, trues.shape)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        # result save\n",
    "        # folder_path = './results/' + setting + '/'\n",
    "        # if not os.path.exists(folder_path):\n",
    "        #     os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe, mase = metric(preds, trues)\n",
    "\n",
    "        print('mse:{}, mae:{}, mase:{}'.format(mse, mae, mase))\n",
    "        # f = open(\"result.txt\", 'a')\n",
    "        # f.write(setting + \"  \\n\")\n",
    "        # f.write('mse:{}, mae:{}'.format(mse, mae))\n",
    "        # f.write('\\n')\n",
    "        # f.write('\\n')\n",
    "        # f.close()\n",
    "\n",
    "        # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe, mase]))\n",
    "        # np.save(folder_path + 'pred.npy', preds)\n",
    "        # np.save(folder_path + 'true.npy', trues)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_i ==  17597\n",
      "0/17597\n",
      "10/17597\n",
      "20/17597\n",
      "30/17597\n",
      "40/17597\n",
      "50/17597\n",
      "60/17597\n",
      "70/17597\n",
      "80/17597\n",
      "90/17597\n",
      "100/17597\n",
      "test shape: (101, 720, 321) (101, 720, 321)\n",
      "mse:0.4629252552986145, mae:0.3926493525505066, mase:1.2603224515914917\n"
     ]
    }
   ],
   "source": [
    "test(autoformer_elec, dataloader_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 2)\n",
      "(1, 7, 2)\n",
      "MASE: 4.357142857142857\n"
     ]
    }
   ],
   "source": [
    "# sanity check for MASE function\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def MAE(pred, true):\n",
    "#     return np.mean(np.abs(pred - true))\n",
    "\n",
    "# def MASE(pred, true):\n",
    "#     y_naive = np.roll(true, 1, axis=1)  # Naive forecast (shifted by one time step)\n",
    "\n",
    "#     return MAE(pred, true)/ MAE(y_naive[0,1:,:], true[0,1:,:])\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming you have actual values (y_true), predicted values (y_pred)\n",
    "# y_true = np.array([[[1,8],[2,9],[3,10],[4,11],[5,12],[6,13],[7,14]]])\n",
    "# y_pred = np.array([[[0,0],[5,5],[8,8],[5,5],[3,3],[8,8],[4,4]]])\n",
    "\n",
    "# print(y_true.shape)\n",
    "# print(y_pred.shape)\n",
    "\n",
    "# mase_value = MASE(y_pred, y_true)\n",
    "# print(f\"MASE: {mase_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part\n",
    "In this part we compare the number of parameters for the linear model and the autoformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoformer\n",
    "from utils.tools import dotdict\n",
    "import torch\n",
    "\n",
    "def obtain_autoformer(pred_len, dataset):\n",
    "    assert dataset in [\"ECL\", \"Exchange\"]\n",
    "    assert pred_len in [96, 192, 336, 720]\n",
    "\n",
    "    args = dotdict()\n",
    "    args.pred_len = pred_len\n",
    "\n",
    "    if dataset == \"ECL\":\n",
    "        args.enc_in = 321\n",
    "        args.dec_in = 321\n",
    "        args.c_out = 321\n",
    "    elif dataset == \"Exchange\":\n",
    "        args.enc_in = 8\n",
    "        args.dec_in = 8\n",
    "        args.c_out = 8\n",
    "\n",
    "    args.target = 'OT'\n",
    "    args.des = 'train'\n",
    "    args.dropout = 0.05\n",
    "    args.num_workers = 10\n",
    "    args.gpu = 0\n",
    "    args.lradj = 'type1'\n",
    "    args.devices = '0'\n",
    "    args.use_gpu = False\n",
    "    args.use_multi_gpu = False\n",
    "    args.freq = 'h'\n",
    "    args.checkpoints = './checkpoints/'\n",
    "    args.bucket_size = 4\n",
    "    args.n_hashes = 4\n",
    "    args.is_trainging = True\n",
    "    args.data = 'custom'\n",
    "    args.features = 'M'\n",
    "    args.seq_len = 96\n",
    "    args.label_len = 48\n",
    "    args.e_layers = 2\n",
    "    args.d_layers = 1\n",
    "    args.n_heads = 8\n",
    "    args.factor = 1\n",
    "    args.d_model = 512\n",
    "    args.des = 'Exp'\n",
    "    args.itr = 1\n",
    "    args.d_ff = 2048\n",
    "    args.moving_avg = 25\n",
    "    args.factor = 3\n",
    "    args.distil = True\n",
    "    args.output_attention = False\n",
    "    args.embed = 'timeF'\n",
    "\n",
    "    autoformer_path = f\"/Users/angelavansprang/Documents/PhD/transformers for time series/Autoformer/checkpoints/{dataset}_96_{pred_len}_Autoformer_custom_ftM_sl96_ll48_pl{pred_len}_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\"\n",
    "\n",
    "    autoformer = Autoformer.Model(args).float()\n",
    "    autoformer.load_state_dict(torch.load(autoformer_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    autoformer.eval()\n",
    "\n",
    "    return autoformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Linear\n",
    "from utils.tools import dotdict\n",
    "import torch\n",
    "\n",
    "def obtain_linear(pred_len, dataset):\n",
    "    assert dataset in [\"ECL\", \"Exchange\"]\n",
    "    assert pred_len in [96, 192, 336, 720]\n",
    "\n",
    "    args = dotdict()\n",
    "    args.pred_len = pred_len\n",
    "\n",
    "    args.target = 'OT'\n",
    "    args.des = 'train'\n",
    "    args.dropout = 0.05\n",
    "    args.num_workers = 10\n",
    "    args.gpu = 0\n",
    "    args.lradj = 'type1'\n",
    "    args.devices = '0'\n",
    "    args.use_gpu = False\n",
    "    args.use_multi_gpu = False\n",
    "    args.freq = 'h'\n",
    "    args.checkpoints = './checkpoints/'\n",
    "    args.bucket_size = 4\n",
    "    args.n_hashes = 8\n",
    "    args.is_trainging = True\n",
    "    args.data = 'custom'\n",
    "    args.features = 'M'\n",
    "    args.seq_len = 336\n",
    "    args.label_len = 48\n",
    "    args.factor = 1\n",
    "\n",
    "\n",
    "    args.d_model = 512\n",
    "    args.des = 'Exp'\n",
    "    args.itr = 1\n",
    "    args.d_ff = 2048\n",
    "    args.moving_avg = 25\n",
    "    args.factor = 1\n",
    "    args.distil = True\n",
    "    args.output_attention = False\n",
    "    args.embed = 'timeF'\n",
    "\n",
    "    linear_path = f\"/Users/angelavansprang/Documents/PhD/transformers for time series/Autoformer/checkpoints/{dataset}_{args.seq_len}_{pred_len}_Linear_custom_ftM_sl{args.seq_len}_ll48_pl{pred_len}_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0/checkpoint.pth\"\n",
    "\n",
    "    linear = Linear.Model(args).float()\n",
    "    linear.load_state_dict(torch.load(linear_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    linear.eval()\n",
    "\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoformer_96_electricity = obtain_autoformer(pred_len=720, dataset=\"Exchange\")\n",
    "linear_96_electricity = obtain_linear(pred_len=720, dataset=\"Exchange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoformer: 10541064\n",
      "linear: 242640\n"
     ]
    }
   ],
   "source": [
    "print(f\"autoformer: {count_parameters(autoformer_96_electricity)}\")\n",
    "print(f\"linear: {count_parameters(linear_96_electricity)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5eefa6bba9495c8bf9f26e41ac1884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c476cfa40aa244b89b196fea5f22cb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/116k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "\n",
    "config = AutoformerConfig.from_pretrained(\"kashif/autoformer-traffic-hourly\")\n",
    "model = AutoformerForPrediction.from_pretrained(\"kashif/autoformer-traffic-hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoformerForPrediction(\n",
       "  (model): AutoformerModel(\n",
       "    (scaler): AutoformerMeanScaler()\n",
       "    (encoder): AutoformerEncoder(\n",
       "      (value_embedding): AutoformerValueEmbedding(\n",
       "        (value_projection): Linear(in_features=47, out_features=16, bias=False)\n",
       "      )\n",
       "      (embed_positions): AutoformerSinusoidalPositionalEmbedding(72, 16)\n",
       "      (layers): ModuleList(\n",
       "        (0): AutoformerEncoderLayer(\n",
       "          (self_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (final_layer_norm): AutoformerLayernorm(\n",
       "            (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decomp1): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp2): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (1): AutoformerEncoderLayer(\n",
       "          (self_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (final_layer_norm): AutoformerLayernorm(\n",
       "            (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decomp1): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp2): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): AutoformerDecoder(\n",
       "      (value_embedding): AutoformerValueEmbedding(\n",
       "        (value_projection): Linear(in_features=47, out_features=16, bias=False)\n",
       "      )\n",
       "      (embed_positions): AutoformerSinusoidalPositionalEmbedding(72, 16)\n",
       "      (layers): ModuleList(\n",
       "        (0): AutoformerDecoderLayer(\n",
       "          (self_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (final_layer_norm): AutoformerLayernorm(\n",
       "            (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decomp1): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp2): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp3): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (trend_projection): Conv1d(16, 47, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "        )\n",
       "        (1): AutoformerDecoderLayer(\n",
       "          (self_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): AutoformerAttention(\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (final_layer_norm): AutoformerLayernorm(\n",
       "            (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decomp1): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp2): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (decomp3): AutoformerSeriesDecompositionLayer(\n",
       "            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "          (trend_projection): Conv1d(16, 47, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (seasonality_projection): Linear(in_features=16, out_features=47, bias=True)\n",
       "    )\n",
       "    (decomposition_layer): AutoformerSeriesDecompositionLayer(\n",
       "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (parameter_projection): ParameterProjection(\n",
       "    (proj): ModuleList(\n",
       "      (0): Linear(in_features=47, out_features=1, bias=True)\n",
       "      (1): Linear(in_features=47, out_features=1, bias=True)\n",
       "      (2): Linear(in_features=47, out_features=1, bias=True)\n",
       "    )\n",
       "    (domain_map): LambdaLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
